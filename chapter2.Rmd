# Regression and model validation

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

### 1. Read in and describe the data
```{r}
learning2014 <- read.csv("learning2014.csv")
str(learning2014)
dim(learning2014)
```
The data consists of 166 observations for 7 variables (the 8th variable is merely an index here). The data has been collected over the course _Introduction to Social Statistics_ in 2014. Non-selfexplanatory columns are:

* "deep": Deep learning evaluation 
* "stra": Strategic learning evaluation
* "surf"; Surface learning evaluation

More details available at http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt

### 2. Graphical overview of the data
```{r}
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
```
```{r}
summary(learning2014)
```
Blue represents data where gender is "male", red where gender is "female". The table overview shows that the data consists to only one third of data with gender being "male", The graphical overview shows that higher points correlate with a better attitude.

### 3. Regression model

```{r}
my_model2 <- lm(Points ~ Attitude + stra + Age , data = learning2014)
summary(my_model2)

```
The summary shows the results of two statistical tests. First of all the **t-value** tell us about how far our estimated parameter is from a hypothesized 0. This the highest for "stra". The **p-value** (Pr) tells us about the probability of observing a value at least as extreme as our coefficient and usually should be below a 0.05 threshold.
The summary of the model shows that only "Attitude" has a low enough p-value to reject the null hypothesis and therefore has a significant relationship with our target "Points".
Therefore we drop the features "stra" and "Age" and rerun the model: 
```{r}
my_model2 <- lm(Points ~ Attitude, data = learning2014)
summary(my_model2)

```
Now our model is statistically significant.

### 4. Interpretation of the model
The intercept tells us that someone with 0 attitude will have 11.6 points. From there for each "Attitude" increment the "Points" will rise by 0.35. Someone with the maximum of 50 in attitude will have a predicted point score of 17 + 11 = 28. We have a linear relationship here. The multiple r-squared is always between 0 and 1 and the higher the more variation in the response variable is explained by the model. The same is true for the adjusted r-squared just that it takes into account the number of independent variables. Our model is therefore not well suited to explain this variation as the value is only 0.19.

### 5. Plot Residuals

```{r}
plot(my_model2, which = 1)
```

Ideally the residual would be as close to 0 as possible. We have a quite big spreading, which is also reflected by our poor r-squared.

The normal Q-Q plot plots the quantiles of two different probability distributions against each other. Ideally all the values are on the diagonal:
```{r}
plot(my_model2, which = 2)
```

We can see that for the outer quantiles the values diverge from the quantiles.

_Leverage_ describes how far a covariate is from other covariates, respectively for the present assumed linear regression it measures how sensitive a predicted value is to a change of the corresponding true value:
```{r}
plot(my_model2, which = 5)
```

The points are more or less evenly spread.