# 4. Clustering and Classification


### 2. Load and describe the data
```{r}
library(MASS)
data("Boston")
str(Boston)
```
The data contains information about housing values in suburbs of Boston. It contains information regarding crime rates, proportions of residental land/non-retail and various other variables influencing housing values, detailed information here:  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html
There are **506** observables of **14** variables.

### 3. Graphical overview

For obtaining an graphical overview of the data it is here useful to plot a **Correlation matrix** additional to the summary.
```{r}
library(dplyr)
library(tidyverse)
library(corrplot)
summary(Boston)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```
*crim*, that is crime rates, is the only variable the spans over several order of magnitudes. Otherwise most of the data is in a range of tens and hundreds. There is lots of correlation within the data. Only exception is *chas*, (Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)). Also *black* (measure for the proprtion of black population) shows rather small correlationss. We have various clear correlations that feel intuitively right, such as *indus* (industry proportion) with *nox* (nitrogen oxide concentration) or a negative correlation of *lstat* (lower status of populatrion in %) to *medv* (median value of homes).

### 4. Standardize Data

We will standardize the data: 
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```
Standardizing set all data means to zero.
Now we turn *crim* into a categorical variable, based on its quantiles. Finally we will put 80% of the data into a train set and the rest to a testset.
```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(Boston)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```
The categorical *crime* data is split into four categories of low, medium low, medium high and high crime rates.

### 5. Linear Discriminant Analysis

Let*s fit a Linear Discriminant Analysis model on the data:

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```
Especially the high and medium high crime rates look classified together. Nevertheless, in this particular plot low and medium low seem very mixed.

### 6. Predictions onto the testset

Let*s fit a Linear Discriminant Analysis model on the data:

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
We see that high and medium high was all correct predicted with one exception. However, only roughly a third of medium low was correctly predicted, where one other third was misspredicted as low and one as medium high, this is what we expected from the figure above. Also One third of "low" was misspredicted as medium low.

### 7. Data Distances

Let's reload the dataset and calculate distances between the variables and then run the k-means algorithm
```{r}
# load MASS and Boston
library(MASS)
data('Boston')

boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled[6:10], col = km$cluster)
```

With 3 clusters we get some distinction of the datapoints, however, the graph makes clear that it could be separated more and better. Therefore, we observe the total of within cluster sum of squares (WCSS). When this measure is lowest we fidn the optimal number of clusters.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
The graph suggests that we may find a better results with 10 clusters. However, that may be a overfit, so we take 8 instead as there is a rapid decline in the curve right before 8 clusters.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 8)

# plot the Boston dataset with clusters
pairs(boston_scaled[6:10], col = km$cluster)
```
Visually the result got worse, because it is way more difficult to achieve any interpretation of the result. With 8 clusters therer is already some overfit. Nevertheless, the brightblue cluster seems to be well described.